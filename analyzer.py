from openai import OpenAI
from typing import List, Dict, Optional
from config import settings
import json

class DocumentAnalyzer:
    """Analisador com o1 (GPT-4.1) para m√°xima qualidade usando RAG"""
    
    def __init__(self, vectorstore=None):
        self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
        self.vectorstore = vectorstore  # Para buscar chunks via RAG
    
    def analyze(self, 
                question: str, 
                chunks: List[Dict],
                custom_prompt: str = None) -> Dict:
        """Analisa documento com o1 e retorna resposta com refer√™ncias"""
        
        # Construir contexto com refer√™ncias de p√°gina
        context = self._build_context(chunks)
        
        # Prompt base (ser√° customizado depois)
        base_prompt = custom_prompt or """Analise o processo jur√≠dico e responda √† pergunta.

INSTRU√á√ïES:
- Responda apenas com base no contexto fornecido
- Cite sempre a p√°gina e arquivo de origem
- Use formato: "...conforme p√°gina X do arquivo Y..."
- Seja preciso e direto
- Se n√£o souber, diga claramente"""

        full_prompt = f"""{base_prompt}

CONTEXTO:
{context}

PERGUNTA: {question}

RESPOSTA:"""

        # Chamar GPT-4.1
        response = self.client.chat.completions.create(
            model=settings.MODEL_O1,
            messages=[
                {"role": "user", "content": full_prompt}
            ]
        )
        
        answer = response.choices[0].message.content
        
        # Extrair refer√™ncias automaticamente
        references = self._extract_references(chunks)
        
        return {
            "answer": answer,
            "references": references,
            "chunks_used": len(chunks),
            "model": settings.MODEL_O1
        }
    
    def _build_context(self, chunks: List[Dict]) -> str:
        """Constr√≥i contexto com refer√™ncias claras"""
        context_parts = []
        
        for chunk in chunks:
            part = f"""[Arquivo: {chunk['filename']} | P√°gina: {chunk['page_number']}]
{chunk['content']}
---"""
            context_parts.append(part)
        
        return "\n\n".join(context_parts)
    
    def _extract_references(self, chunks: List[Dict]) -> List[Dict]:
        """Extrai refer√™ncias estruturadas"""
        refs = []
        seen = set()
        
        for chunk in chunks:
            key = (chunk['filename'], chunk['page_number'])
            if key not in seen:
                refs.append({
                    "filename": chunk['filename'],
                    "page": chunk['page_number']
                })
                seen.add(key)
        
        return refs
    
    def analyze_full_document_rag(self, document_id: str, filename: str, prompt_file: str = "prompt_analise.txt", return_raw_response: bool = False) -> Dict:
        """Analisa documento completo usando RAG - busca chunks relevantes por embeddings
        
        Args:
            document_id: ID do documento
            filename: Nome do arquivo
            prompt_file: Caminho para arquivo com prompt completo
        """
        print(f"üü° [ANALYZER] analyze_full_document_rag: Iniciando an√°lise RAG")
        print(f"üü° [ANALYZER] document_id: {document_id}")
        print(f"üü° [ANALYZER] filename: {filename}")
        print(f"üü° [ANALYZER] prompt_file: {prompt_file}")
        print(f"üü° [ANALYZER] return_raw_response: {return_raw_response}")
        
        if not self.vectorstore:
            print(f"‚ùå [ANALYZER] ERRO: VectorStore n√£o inicializado")
            raise ValueError("VectorStore n√£o inicializado. Passe vectorstore no __init__")
        
        # Carregar prompt
        print(f"üü° [ANALYZER] Carregando prompt do arquivo {prompt_file}...")
        try:
            with open(prompt_file, "r", encoding="utf-8") as f:
                full_prompt_template = f.read()
            prompt_len = len(full_prompt_template)
            print(f"‚úÖ [ANALYZER] Prompt carregado: {prompt_len} caracteres")
        except FileNotFoundError:
            print(f"‚ùå [ANALYZER] ERRO: Arquivo {prompt_file} n√£o encontrado")
            raise FileNotFoundError(f"Arquivo {prompt_file} n√£o encontrado")
        
        # Queries para busca RAG - cobrem todos os temas das perguntas
        rag_queries = [
            "desconsidera√ß√£o da personalidade jur√≠dica proced√™ncia improced√™ncia",
            "desconsidera√ß√£o liminar contradit√≥rio produ√ß√£o de provas",
            "artigo 50 CC artigo 28 CDC artigo 795 CLT fundamenta√ß√£o jur√≠dica",
            "desconsidera√ß√£o patrim√¥nio proporcional bloqueio integral bens",
            "grau de prova exigido ind√≠cios graves presun√ß√£o desconsidera√ß√£o",
            "prova documental pericial testemunhal indici√°ria desconsidera√ß√£o",
            "prova outros processos invent√°rio fal√™ncia processo c√≠vel",
            "invers√£o √¥nus da prova terceiro boa-f√© desconsidera√ß√£o",
            "terceiro de boa-f√© prote√ß√£o adquirente bens executado",
            "requisitos boa-f√© terceiro desconhecimento pre√ßo mercado registro p√∫blico",
            "boa-f√© momento contrata√ß√£o pagamentos conhecimento a√ß√µes judiciais",
            "consultas pr√©vias certid√µes pesquisas processos √≥rg√£os cr√©dito boa-f√©",
            "opera√ß√µes familiares c√¥njuges presun√ß√£o fraude desconsidera√ß√£o",
            "pagamentos terceiros vinculados d√≠vidas im√≥vel acordos trabalhistas",
            "rastreabilidade valores pagos comprova√ß√£o credores finais",
            "cl√°usulas contratuais direcionamento pagamentos terceiros simula√ß√£o fraude",
            "aquisi√ß√£o antes depois ajuizamento tr√¢nsito julgado desconsidera√ß√£o",
            "prazo decadencial prescricional desconsidera√ß√£o personalidade jur√≠dica",
            "efeitos retroativos prospectivos desconsidera√ß√£o opera√ß√µes passadas",
            "Tema 1232 STF suspens√£o processos desconsidera√ß√£o",
            "pre√ßo vil laudo pericial avaliat√≥rio subavalia√ß√£o bem",
            "percentual des√°gio valor mercado negocia√ß√µes aceit√°vel",
            "condi√ß√µes im√≥vel conserva√ß√£o ocupa√ß√£o √¥nus urg√™ncia venda",
            "valor mercado data transa√ß√£o data atual per√≠cia pre√ßo vil",
            "descumprimento ordem judicial multa presun√ß√£o m√°-f√© invers√£o √¥nus",
            "ordens judiciais outros processos vinculam terceiros trabalhistas",
            "excludente responsabilidade descumprimento ordem obriga√ß√µes contratuais conflitantes",
            "confus√£o patrimonial contas banc√°rias contabilidade separada uso prom√≠scuo bens",
            "confus√£o patrimonial pessoas jur√≠dicas grupo econ√¥mico s√≥cios",
            "confus√£o patrimonial contempor√¢nea execu√ß√£o hist√≥rica regularizada",
            "desvio finalidade social pessoa jur√≠dica ultra vires objeto social",
            "grupo econ√¥mico informal fato estrutura societ√°ria formal",
            "grupo econ√¥mico dire√ß√£o √∫nica coordena√ß√£o administrativa solidariedade interesses",
            "prova grupo econ√¥mico documentos societ√°rios demonstra√ß√µes financeiras contratos",
            "fraude execu√ß√£o art 792 CPC fraude contra credores art 158 CC",
            "fraude execu√ß√£o bem im√≥vel penhora registrada matr√≠cula aliena√ß√£o",
            "conhecimento demanda trabalhista vendedor ci√™ncia inequ√≠voca adquirente",
            "aliena√ß√£o tornou devedor insolvente presun√ß√£o insolv√™ncia d√≠vidas trabalhistas",
            "terceiro assume acordos trabalhistas vendedor pre√ßo aquisi√ß√£o fraude",
            "esgotamento bens executado principal desconsiderar personalidade jur√≠dica terceiros",
            "√¥nus provar insufici√™ncia patrimonial executado exequente terceiro",
            "pesquisas patrimoniais RENAJUD BACENJUD INFOJUD SISBAJUD desconsidera√ß√£o",
            "garantias alternativas seguro garantia fian√ßa banc√°ria cau√ß√£o substituir penhora",
            "limita√ß√£o penhora valor necess√°rio garantir execu√ß√£o liberar excesso",
            "intima√ß√£o pr√©via terceiro Tema 916 STJ desconsidera√ß√£o liminar urg√™ncia",
            "prazo manifesta√ß√£o terceiro inclu√≠do 15 dias urg√™ncia complexidade",
            "dila√ß√£o probat√≥ria per√≠cias testemunhas documentos incidente desconsidera√ß√£o",
            "solu√ß√µes conciliat√≥rias desconsidera√ß√£o audi√™ncias acordos",
            "acordos desconsidera√ß√£o parcelamento d√©bito da√ß√£o bem divis√£o proporcional",
            "momento adequado propostas acordo fase processual negocia√ß√£o",
            "decis√µes desconsidera√ß√£o reformadas TRT-2 agravo peti√ß√£o",
            "fundamentos TRT-2 reformar desconsidera√ß√£o contradit√≥rio insufici√™ncia probat√≥ria",
            "efeito suspensivo agravo peti√ß√£o decis√µes desconsidera√ß√£o execu√ß√£o imediata",
            "tempo m√©dio agravo peti√ß√£o julgamento TRT-2 casos similares",
            "nulidade neg√≥cio jur√≠dico imobili√°rio v√≠cio origem procura√ß√£o suspensa compet√™ncia",
            "nulidade t√≠tulo origem terceiro boa-f√© prote√ß√£o terceiro invalidade ato",
            "nulidade neg√≥cio jur√≠dico restitui√ß√£o status quo confian√ßa enriquecimento",
            "decis√µes invent√°rio vinculantes relevantes processos independentes",
            "direitos esp√≥lio inventariante terceiro adquirente boa-f√© conflito",
            "aliena√ß√£o bem empresa indeniza√ß√£o cotas sociais herdeiro fraude",
            "fraude fiscal pagamentos contabiliza√ß√£o fundamento desconsidera√ß√£o irregularidade",
            "irregularidades cont√°beis tribut√°rias per√≠cia cont√°bil alega√ß√µes gen√©ricas",
            "compet√™ncia analisar quest√µes tribut√°rias complexas prejudicial Justi√ßa Federal"
        ]
        
        # Buscar chunks relevantes usando RAG (m√∫ltiplas queries para cobrir todos os temas)
        print(f"üü° [ANALYZER] Iniciando busca RAG com {len(rag_queries)} queries tem√°ticas")
        all_relevant_chunks = []
        seen_chunk_ids = set()
        
        # Buscar chunks para cada query tem√°tica
        for query_idx, query in enumerate(rag_queries, 1):
            if query_idx <= 5 or query_idx % 10 == 0:  # Log a cada 10 queries para n√£o poluir
                print(f"üü° [ANALYZER] Buscando chunks para query {query_idx}/{len(rag_queries)}: '{query[:50]}...'")
            
            chunks = self.vectorstore.search(query, document_id=document_id, limit=10)
            
            if query_idx <= 5 or query_idx % 10 == 0:
                print(f"üü° [ANALYZER] Query {query_idx}: {len(chunks)} chunks encontrados")
            
            new_chunks_count = 0
            for chunk in chunks:
                chunk_id = chunk.get('id')
                if chunk_id and chunk_id not in seen_chunk_ids:
                    all_relevant_chunks.append(chunk)
                    seen_chunk_ids.add(chunk_id)
                    new_chunks_count += 1
            
            if query_idx <= 5 or query_idx % 10 == 0 and new_chunks_count > 0:
                print(f"üü° [ANALYZER] Query {query_idx}: {new_chunks_count} novos chunks adicionados (total acumulado: {len(all_relevant_chunks)})")
        
        print(f"üü° [ANALYZER] Ap√≥s queries tem√°ticas: {len(all_relevant_chunks)} chunks √∫nicos encontrados")
        
        # Se n√£o encontrou chunks suficientes, buscar mais genericamente
        if len(all_relevant_chunks) < 50:
            print(f"‚ö†Ô∏è [ANALYZER] Poucos chunks encontrados ({len(all_relevant_chunks)} < 50), buscando genericamente...")
            generic_queries = [
                "senten√ßa decis√£o ac√≥rd√£o",
                "juiz desembargador vara trabalho",
                "processo n√∫mero decis√£o",
                "fundamenta√ß√£o jur√≠dica artigo lei"
            ]
            
            for gen_query_idx, query in enumerate(generic_queries, 1):
                print(f"üü° [ANALYZER] Busca gen√©rica {gen_query_idx}/{len(generic_queries)}: '{query}'")
                chunks = self.vectorstore.search(query, document_id=document_id, limit=20)
                print(f"üü° [ANALYZER] Busca gen√©rica {gen_query_idx}: {len(chunks)} chunks retornados")
                
                new_chunks = 0
                for chunk in chunks:
                    chunk_id = chunk.get('id')
                    if chunk_id and chunk_id not in seen_chunk_ids:
                        all_relevant_chunks.append(chunk)
                        seen_chunk_ids.add(chunk_id)
                        new_chunks += 1
                        if len(all_relevant_chunks) >= 100:  # Limite razo√°vel
                            break
                
                print(f"üü° [ANALYZER] Busca gen√©rica {gen_query_idx}: {new_chunks} novos chunks (total: {len(all_relevant_chunks)})")
                
                if len(all_relevant_chunks) >= 100:
                    print(f"üü° [ANALYZER] Limite de 100 chunks atingido, parando busca")
                    break
        
        print(f"‚úÖ [ANALYZER] Total de chunks √∫nicos coletados: {len(all_relevant_chunks)}")
        
        if not all_relevant_chunks:
            print(f"‚ùå [ANALYZER] ERRO: Nenhum chunk encontrado para documento {document_id}")
            raise ValueError(f"Nenhum chunk encontrado para documento {document_id}")
        
        # Ordenar chunks por p√°gina para manter ordem l√≥gica
        print(f"üü° [ANALYZER] Ordenando chunks por p√°gina e chunk_id...")
        all_relevant_chunks.sort(key=lambda x: (x.get('page_number', 0), x.get('chunk_id', 0)))
        print(f"‚úÖ [ANALYZER] Chunks ordenados")
        
        # Construir contexto com chunks relevantes
        print(f"üü° [ANALYZER] Construindo contexto com {len(all_relevant_chunks)} chunks...")
        context = self._build_context(all_relevant_chunks)
        context_len = len(context)
        print(f"‚úÖ [ANALYZER] Contexto constru√≠do: {context_len} caracteres")
        
        # Montar prompt final
        print(f"üü° [ANALYZER] Montando prompt final...")
        prompt_with_context = f"""{full_prompt_template}

# DOCUMENTO PARA AN√ÅLISE

{context}

---

Agora analise o documento acima e forne√ßa as respostas no formato especificado."""
        
        final_prompt_len = len(prompt_with_context)
        print(f"‚úÖ [ANALYZER] Prompt final montado: {final_prompt_len} caracteres ({final_prompt_len/1000:.2f}K chars)")
        
        # Chamar GPT-4.1
        print(f"üü° [ANALYZER] Chamando GPT-4.1 ({settings.MODEL_O1}) para an√°lise...")
        print(f"üü° [ANALYZER] Aguardando resposta da API...")
        
        try:
            response = self.client.chat.completions.create(
                model=settings.MODEL_O1,
                messages=[
                    {"role": "user", "content": prompt_with_context}
                ]
            )
            
            answer = response.choices[0].message.content
            answer_len = len(answer) if answer else 0
            print(f"‚úÖ [ANALYZER] Resposta GPT-4.1 recebida: {answer_len} caracteres ({answer_len/1000:.2f}K chars)")
            
            if hasattr(response, 'usage'):
                usage = response.usage
                print(f"üü° [ANALYZER] Uso de tokens: prompt_tokens={getattr(usage, 'prompt_tokens', 'N/A')}, completion_tokens={getattr(usage, 'completion_tokens', 'N/A')}, total_tokens={getattr(usage, 'total_tokens', 'N/A')}")
        except Exception as e:
            print(f"‚ùå [ANALYZER] ERRO ao chamar GPT-4.1: {str(e)}")
            print(f"‚ùå [ANALYZER] Tipo do erro: {type(e).__name__}")
            import traceback
            print(f"‚ùå [ANALYZER] Traceback: {traceback.format_exc()}")
            raise
        
        # Parsear resposta estruturada
        print(f"üü° [ANALYZER] Parseando resposta da an√°lise...")
        parsed_data = self._parse_analysis_response(answer, filename, all_relevant_chunks)
        
        result_keys = len(parsed_data.keys()) if parsed_data else 0
        print(f"‚úÖ [ANALYZER] Resposta parseada: {result_keys} campos extra√≠dos")
        if parsed_data:
            print(f"üü° [ANALYZER] Campos extra√≠dos: {list(parsed_data.keys())[:10]}... (mostrando primeiros 10)")
            if 'numero_processo' in parsed_data:
                print(f"üü° [ANALYZER] N√∫mero do processo: {parsed_data['numero_processo']}")
        
        if return_raw_response:
            print(f"üü° [ANALYZER] Retornando resultado parseado + resposta bruta")
            return parsed_data, answer
        else:
            print(f"üü° [ANALYZER] Retornando apenas resultado parseado")
            return parsed_data
    
    def _parse_analysis_response(self, response_text: str, filename: str, chunks: List[Dict]) -> Dict:
        """Extrai dados estruturados da resposta do o1"""
        print(f"üü° [ANALYZER] _parse_analysis_response: Iniciando parseamento")
        print(f"üü° [ANALYZER] response_text length: {len(response_text)} caracteres")
        print(f"üü° [ANALYZER] filename: {filename}")
        print(f"üü° [ANALYZER] chunks dispon√≠veis: {len(chunks)}")
        
        import re
        
        data = {
            "arquivo_original": filename,
            "analisado_por": "Sistema IA (o1)",
            "status_analise": "CONCLUIDO"
        }
        
        # Extrair identifica√ß√£o
        print(f"üü° [ANALYZER] Extraindo identifica√ß√£o (JUIZ, NUMERO_PROCESSO, etc.)...")
        juiz_match = re.search(r'\*\*JUIZ:\*\*\s*(.+)', response_text)
        if juiz_match:
            data["juiz"] = juiz_match.group(1).strip()
            print(f"‚úÖ [ANALYZER] JUIZ extra√≠do: {data['juiz']}")
        else:
            print(f"‚ö†Ô∏è [ANALYZER] JUIZ n√£o encontrado na resposta")
        
        num_proc_match = re.search(r'\*\*NUMERO_PROCESSO:\*\*\s*(\d{7}-\d{2}\.\d{4}\.\d{1}\.\d{2}\.\d{4})', response_text)
        if num_proc_match:
            data["numero_processo"] = num_proc_match.group(1).strip()
            print(f"‚úÖ [ANALYZER] NUMERO_PROCESSO extra√≠do: {data['numero_processo']}")
        else:
            print(f"‚ö†Ô∏è [ANALYZER] NUMERO_PROCESSO n√£o encontrado na resposta")
        
        data_dec_match = re.search(r'\*\*DATA_DECISAO:\*\*\s*(\d{4}-\d{2}-\d{2})', response_text)
        if data_dec_match:
            data["data_decisao"] = data_dec_match.group(1).strip()
        
        tipo_match = re.search(r'\*\*TIPO_DECISAO:\*\*\s*(.+?)(?:\n|$)', response_text)
        if tipo_match:
            data["tipo_decisao"] = tipo_match.group(1).strip()
        
        grau_match = re.search(r'\*\*GRAU:\*\*\s*(1¬∫\s*Grau|2¬∫\s*Grau)', response_text)
        if grau_match:
            data["grau"] = grau_match.group(1).strip()
        
        vara_match = re.search(r'\*\*VARA:\*\*\s*(.+?)(?:\n|$)', response_text)
        if vara_match:
            data["vara"] = vara_match.group(1).strip()
        else:
            data["vara"] = "5¬™ Vara do Trabalho de Barueri"  # Default
        
        tribunal_match = re.search(r'\*\*TRIBUNAL:\*\*\s*(.+?)(?:\n|$)', response_text)
        if tribunal_match:
            data["tribunal"] = tribunal_match.group(1).strip()
        else:
            data["tribunal"] = "TRT 2¬™ Regi√£o"  # Default
        
        # Extrair campos de decis√£o (decisao_resposta, decisao_justificativa, decisao_referencia)
        decisao_resposta_match = re.search(r'\*\*decisao_resposta:\*\*\s*\n?(.*?)(?=\n\*\*decisao_|\n\*\*p\d+_|\n---|\n##|\Z)', response_text, re.DOTALL)
        if decisao_resposta_match:
            data["decisao_resposta"] = decisao_resposta_match.group(1).strip()
        
        decisao_justificativa_match = re.search(r'\*\*decisao_justificativa:\*\*\s*\n?(.*?)(?=\n\*\*decisao_|\n\*\*p\d+_|\n---|\n##|\Z)', response_text, re.DOTALL)
        if decisao_justificativa_match:
            data["decisao_justificativa"] = decisao_justificativa_match.group(1).strip()
        
        decisao_referencia_match = re.search(r'\*\*decisao_referencia:\*\*\s*\n?(.*?)(?=\n\*\*decisao_|\n\*\*p\d+_|\n---|\n##|\Z)', response_text, re.DOTALL)
        if decisao_referencia_match:
            data["decisao_referencia"] = decisao_referencia_match.group(1).strip()
        
        # Extrair todas as respostas (p1_1_resposta, p1_1_justificativa, etc.)
        print(f"üü° [ANALYZER] Extraindo campos de perguntas (p1_1_resposta, p1_1_justificativa, etc.)...")
        # Padr√£o CORRIGIDO: usar p literal (n√£o [p]), e m√∫ltiplas estrat√©gias
        # Estrat√©gia 1: padr√£o espec√≠fico com lookahead para pr√≥ximo campo
        pattern = r'\*\*(p\d+_\d+_(?:resposta|justificativa|referencia)):\*\*\s*\n(.*?)(?=\n\*\*p\d+_\d+_|\n---|\n##|\Z)'
        matches = list(re.finditer(pattern, response_text, re.DOTALL | re.MULTILINE))
        
        print(f"üü° [ANALYZER] Estrat√©gia 1: {len(matches)} matches encontrados")
        
        extracted_fields = []
        for match in matches:
            campo = match.group(1).strip()
            valor = match.group(2).strip()
            
            # Validar se √© um campo v√°lido da tabela
            if campo.startswith('p') and ('_resposta' in campo or '_justificativa' in campo or '_referencia' in campo):
                data[campo] = valor
                extracted_fields.append(campo)
                if len(extracted_fields) <= 5:  # Log apenas os primeiros 5
                    print(f"‚úÖ [ANALYZER] Campo extra√≠do: {campo} (valor: {len(valor)} chars)")
        
        print(f"‚úÖ [ANALYZER] Estrat√©gia 1: {len(extracted_fields)} campos extra√≠dos")
        
        # Se n√£o encontrou suficientes, tentar padr√£o mais flex√≠vel
        if len(extracted_fields) < 30:
            print(f"‚ö†Ô∏è [ANALYZER] Poucos campos extra√≠dos ({len(extracted_fields)} < 30), tentando estrat√©gia 2...")
            # Estrat√©gia 2: padr√£o sem lookahead espec√≠fico
            pattern2 = r'\*\*(p\d+_\d+_(?:resposta|justificativa|referencia)):\*\*\s*\n?(.*?)(?=\n\*\*p\d+_|\n\*\*[A-Z]|\n---|\n##|\Z)'
            matches2 = list(re.finditer(pattern2, response_text, re.DOTALL | re.MULTILINE))
            print(f"üü° [ANALYZER] Estrat√©gia 2: {len(matches2)} matches encontrados")
            
            # Se ainda n√£o encontrou, tentar padr√£o ainda mais flex√≠vel
            if len(extracted_fields) < 20:
                print(f"‚ö†Ô∏è [ANALYZER] Ainda poucos campos ({len(extracted_fields)} < 20), tentando estrat√©gia 3...")
                # Estrat√©gia 3: qualquer campo que comece com **p e tenha n√∫meros
                pattern3 = r'\*\*(p\d+_\d+_(?:resposta|justificativa|referencia)):\*\*\s*\n?(.*?)(?=\n\*\*[a-zA-Z]|\n---|\n##|\Z)'
                matches3 = list(re.finditer(pattern3, response_text, re.DOTALL | re.MULTILINE))
                print(f"üü° [ANALYZER] Estrat√©gia 3: {len(matches3)} matches encontrados")
                matches2.extend(matches3)
            
            new_fields_count = 0
            for match in matches2:
                campo = match.group(1).strip()
                valor = match.group(2).strip()
                
                # Validar campo e evitar duplicatas
                if campo.startswith('p') and campo not in extracted_fields:
                    if '_resposta' in campo or '_justificativa' in campo or '_referencia' in campo:
                        # Limpar valor de espa√ßos em branco excessivos
                        valor_limpo = re.sub(r'\s+', ' ', valor).strip()
                        if valor_limpo and valor_limpo != "N√£o foi poss√≠vel obter par√¢metros para resposta":
                            data[campo] = valor_limpo
                            extracted_fields.append(campo)
                            new_fields_count += 1
                            if new_fields_count <= 5:  # Log apenas os primeiros 5 novos
                                print(f"‚úÖ [ANALYZER] Campo extra√≠do (estrat√©gia 2/3): {campo} (valor: {len(valor_limpo)} chars)")
            
            print(f"‚úÖ [ANALYZER] Estrat√©gias 2/3: {new_fields_count} novos campos extra√≠dos")
        
        total_fields = len(extracted_fields)
        total_data_keys = len(data.keys())
        print(f"‚úÖ [ANALYZER] Parseamento conclu√≠do: {total_fields} campos de perguntas extra√≠dos, {total_data_keys} campos totais no resultado")
        print(f"üü° [ANALYZER] Campos totais no data: {list(data.keys())[:15]}... (mostrando primeiros 15)")
        
        return data